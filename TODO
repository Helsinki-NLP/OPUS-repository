To do as of version 0.56
========================


High priority:

* speed up the detection of align candidates in large data sets
  (import process seems to be stuck for a long time in crawled imports)
* cronjob for cleaning /var/tmp on clients (worker nodes)			(done)
  - see letsmt_weekly.sh
* make langid optional (sent_langid and doc_langid)				(done?)
  ---> option to select tool or none in ImportPara_sent/doc_langid ...
* crawling produces multiple tar-archives to reduce the size			(done)
  - use File::Find to get the files and      	    				(external find is faster)
    split into archives of max 5000 files					(using system split)
    https://stackoverflow.com/questions/17754931/finding-files-with-perl
    https://www.perlmonks.org/?node_id=217166
* get rid of encode/decode and fix all the unicode issues
  - reduce use of encode / decode to avoid all the confusion
  https://sites.google.com/site/kbinstuff/perl-unicode-utf8-cgipm-apache-mod_perl-and-mysql
  https://unix.stackexchange.com/questions/6516/filtering-invalid-utf8
  https://stackoverflow.com/questions/1301402/example-invalid-utf8-string
* better fuzzy matching of file names (translation detection)
  --> lowercased file names
  --> without non-letter characters
  --> remove/replace language names/IDs from names				(done)
  --> complete local settings (sv_SE, fi_FI, en_US, ...)
      https://perldoc.perl.org/Locale/Language.html
      https://github.com/umpirsky/locale-list/tree/master/data
      https://stackoverflow.com/questions/3191664/list-of-all-locales-and-their-short-codes
  --> even match translated names? (difficult)
      (use simple lexical matches with prob's?)
* incremental crawling (only update changed pages)
  - could use file checksums for comparison					(md5 added)
    https://metacpan.org/pod/Digest::MD5::File  this seems optimal!!
    https://metacpan.org/pod/Digest
    https://metacpan.org/pod/Digest::MD4	fastest
    https://metacpan.org/pod/Digest::MD5    	widely used
    https://metacpan.org/pod/File::Signature	specific for files
    https://metacpan.org/pod/File::Checksum
    https://metacpan.org/pod/File::Modified
* TMX options
  - option for removing duplicates						(done = default)
  - large imports (tar-files) etc - send only one de-duplicated TMX		(skip this?)
  - generate one TMX file for one compete subdir tree				(done)
  - exclude incorrect languages (lang attribute in XML)				(done in xces)
    ---> also for xces align files in general!	   				(done)
* sentence alignment also produces TMX files
  - add TMX as export format							(done)
  - integration of TMX files in interface (parallel data to be shown)		(started)
* proper handling of revisions in GIT backend!	    	       			(done)
* allow tags in version-control backends (at least for the git backend)
* check memory leak in sentence alignment?!					(no issue?)
* add dehyphenation module (as normalizer) - see pdf2xml			(done for PDF - enough?)
* integration of srtalign (automatically for srt files?)
  (via external tools and backoff to hunalign if no time-info is found?)
* update language detection (langid.py, cld2)					(done)
  - do we need to check that langhints is among supported languages?
    (see https://github.com/aboSamoor/pycld2)
  https://metacpan.org/pod/Lingua::Identify::CLD2				(does not build)
  - check supported languages before selecting classifier
    https://github.com/CLD2Owners/cld2/issues/58
    https://github.com/aboSamoor/pycld2 (cld2.DETECTED_LANGUAGES)
    https://github.com/saffsd/langid.py
    https://metacpan.org/pod/Lingua::Identify
  - voting for language by several classifiers?
  - also use cld3? (https://github.com/google/cld3)
    python bindings: https://github.com/Elizafox/cld3
* allow manual corrections (alignments and even other things?)			(started)
  --> ISA integration	   	       	   	      				(prototype)
  --> limit the number of sentalign files that can be open for edit
  --> setup ISA									(done)
  --> upload from ISA (overwrite?)						(done)
  --> remove corpus from ISA							(done)
  --> color-coded confidence in ISA
  --> mark corpus that is already done/saved with ISA
  --> keep confidence scores for links that are not changed in ISA
  --> interactive dependency annotation						(started)
* user interface								(on-going)
  - OPUS integration
  - admin interface
  - show logos of sponsors, link to research group, university, OPUS (footer)


Medium priority:

* find translated documents based on their contents (difficult)
  --> use external tools for searching parallel documents?
  --> multilingual sentence-embedding based methods
      see https://arxiv.org/pdf/1812.10464.pdf
      software: https://github.com/facebookresearch/LASER
* integrate tidy to clean up xml problems (e.g. from pdf2xml)
  https://metacpan.org/release/HTML-Valid
* use Encode::Guess for guessing character encoding?				(not very useful)
* nicer look and feel for the repository interface using bootstrap?
  https://getbootstrap.com
* support polyglot python library?
  https://github.com/aboSamoor/polyglot
* use scrapy for web crawling?
  https://docs.scrapy.org/en/latest/intro/overview.html
* corpus tools from http://corpus.tools/wiki
  - newer chared
  - JustText
  - text web crawler spiderling
  - unitok - universal tokenizer
  - de-duplication
  - wiki2corpus (can tika convert wikimedia and other markup langs?)
* support of e-book formats (epub), 
  converter from Calibre (ebook-convert) or Apache-Tika				(done with Tika?)
* jobs for web-crawling (using bitextor, e.g. bitextor integration)
  - wget based crawling								(done)
  - wget2: mime-type based filtering!						(do we need this?)
    https://unix.stackexchange.com/questions/53397/wget-how-to-download-recursively-and-only-specific-mime-types-extensions-i-e/56895
    https://gitlab.com/gnuwget/wget2
  - patch for wget for mime-type based accept:
    https://savannah.gnu.org/bugs/?20378
* interactive dependency aligner (IDA), integrated in interface			(started)
* improve git with remote servers (do not always push!)
* more sentence aligners (srtalign, BLEU align, gargantua, yalign?)
* support moving files (i.e. git mv in backend)
* improved pre-processing tools
  - UDpipe									(started)
  - optimized sentence splitters, tokenizers
  - boilerplate removal etc (e.g. http://corpus.tools/wiki/Justext)
    https://metacpan.org/pod/Text::Identify::BoilerPlate
    boilerpipe (see bitextor)
* slurm job manager (parameters, resource management missing)			(basic mode done)
  --> distributed setup		 	  	     				(started)
  --> resource configuration and different queues?
* integrate bicleaner (or similar tools)?
  https://github.com/bitextor/bicleaner
  pre-trained models from ParaCrawl
* integrate other tokenizers for specific languages
  (mecab for ja ...)
* code optimization (backends, validation, conversion ...)
* profiling the code with https://metacpan.org/pod/Devel::NYTProf		(partially done)
  https://metacpan.org/release/Devel-NYTProf
  https://metacpan.org/pod/Devel::NYTProf::Apache


Low priority:

* use poppler bindings for PDF (https://metacpan.org/pod/Poppler)
  --> also in pd2xml!! (check also inline java for pdfxtk)
  --> Poppler may not include text extraction tools in the library?!
  https://github.com/tamirhassan/pdfxtk/blob/master/pdfXtk/src/main/java/at/ac/tuwien/dbai/pdfwrap/ProcessFile.java
* better bindings of chared (python bindings?) + newer version?
  http://corpus.tools/wiki/Chared
  via Inline::Python?
  https://metacpan.org/pod/distribution/Inline-Python/Python.pod
* git backend									(done)
  - user-subtrees as branches and not copies					(done)
  - remote repositories (master), 
  - user-branches as local copies in compressed filesystem?
  - remote on save server?
* better bindings for git with (do we need that?)
  https://metacpan.org/pod/Git
  https://metacpan.org/pod/Git::Class
  https://metacpan.org/pod/Git::Raw
  https://metacpan.org/pod/Git::XS
* trainable sentence splitters & tokenizers (openNLP, other?)
* automatic load-balancing in distributed setups
* integrate stanford core NLP?
  server: https://stanfordnlp.github.io/CoreNLP/corenlp-server.html


Done:

* import URLs (retrieve from web and convert)					(done)
* jobs for starting word alignment and parsing					(done)
* option to send import result by e-mail
  simple setup of mail utilities: https://tecadmin.net/bash-mail-command-not-found/
  sudo apt-get install mailutils
  echo "Message Body" | mail -s "Message Subject" receiver@example.com
  or using sendmail
  sudo apt-get install sendmail
  sudo apt-get install libemail-sender-perl 
  for perl: https://metacpan.org/pod/Mail::Sendmail
  or better libmime-lite-perl (Mime::Lite)
* integrate pdf2xml								(done)
  (need to improve pdf2xml! too slow! use Tika server!)
* install and setup Apache Tika as a server and call it from Apache::Tika 	(done!)
  https://stackoverflow.com/questions/31733709/auto-restart-a-tika-server
  https://metacpan.org/pod/Apache::Tika
* add call to job API to start import for individual files			(already works!)
* improve the way parallel documents are found					(done)
  --> more efficient! (already for identicial names)				(using String::Approx)
  --> some fuzzy matching on names (but not the complicated way it is now!)
* (semi-)automatic identification of parallel documents (without requiring identical file names)
  --> fuzzi name matching is already in place?! --> bad implementation!
  check string comparison matching modules like
  https://metacpan.org/pod/String::Approx
  https://metacpan.org/pod/release/GRAY/Tree-Suffix-0.21/lib/Tree/Suffix.pm
  https://stackoverflow.com/questions/20584536/longest-common-substring-across-multiple-sequences
  https://metacpan.org/pod/release/JOESUF/Algorithm-LCS-1.04/LCS.pm
  https://metacpan.org/pod/Algorithm::Diff
* update Apache Tika (plus allow options?)					(done, no options)



